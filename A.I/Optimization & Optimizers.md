#### Optimization & Optimizers

```
Optimization : 손실률이 최소인 값을 찾아가는 최적화
Optimizers : 최적화 알고리즘, 머신러닝 학습단계에서 최적의 매개변수를 찾아내는 것이 옵티마이저 역할인데 여기서 최적이란 손실 함수가 최소값이 될 때의 매개변수 값을 의미합니다. 
그러나 일번적인 문제의 손실함수는 매우 복잡하므로 이런 상황에서 기울기를 잘 이용하여 함수의 최소값을 가지는 것이 Optimizers 입니다.
```

optimizers종류
- 경사하강법(Gradient Descent)
- 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)
- Momentum
- Nesterov Accelerated Gradient(NAG)
- Adam
- AdaGrad
- RMSProp
- AdaMax
- Nadam

출처 : https://excelsior-cjh.tistory.com/159, https://velog.io/@freesky/Optimizer, https://james-scorebook.tistory.com/entry/%EC%98%B5%ED%8B%B0%EB%A7%88%EC%9D%B4%EC%A0%80Optimizer-12
